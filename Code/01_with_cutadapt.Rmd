a---
title: "06_practice"
author: "Sarah Yerrace"
date: "2023-05-03"
output: html_document
---

This script is the start of gut content metabarcoding pipeline

Following
https://benjjneb.github.io/dada2/tutorial.html
and using scripts provided by Dr. Jordan Casey

This protocol is for paired-end demultiplexed miseq sequences that have 
sufficient overlap to merge R1 and R2

# Set Up

Load the necessary packages

```{r}
install.packages("digest")
install.packages("tidyverse")
install.packages("seqinr")
install.packages("ape")
install.packages("ade4")
install.packages("filesstrings")


library(dada2); packageVersion("dada2")
library(digest)
library(dplyr)
library(phyloseq)
library(tidyverse)
library(data.table)
library(seqinr)
library(ape)
library(DECIPHER)
library(ade4)
library(janitor)
library(vegan)
library(filesstrings)
library(BiocManager)
library(ShortRead)
```

# Check the Starting Quality

Path to unedited fastq files

```{r}
reads.to.trim <- "../Data/Raw"
list.files(reads.to.trim)
```

Sorting forward and reverse reads based on file name pattern

```{r}

fnFs <- sort(list.files(reads.to.trim, pattern=".R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(reads.to.trim, pattern=".R2.fastq", full.names = TRUE))

sample.names <- str_replace(basename(fnFs), ".1.R1.fastq","")

```

This shows a plot of the quality of the forward reads

In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line).

```{r}
plotQualityProfile(fnFs[1:143],aggregate = TRUE)
```

This plot shows quality of the reverse reads

```{r}
plotQualityProfile(fnRs[1:143],aggregate = TRUE)
```

# Use CutAdapt to remove Primers

This is the path to cut adapt. I don't have permission to run on raven but I can run this on my laptop with appropriate directory

```{r}
cutadapt <- "../Applications/cutadapt.exe"
```

These are the COI primers. Taken from methods txt file from Jonah ventures.
The Function allOrients makes forward, compliment, reverse, and reverse compliment for both primers. 
This is to look for the primers in the sequences in any orientation (PrimerHits, the next function/ chunck)

```{r}
FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"
REV <- "TANACYTCNGGRTGNCCRAARAAYCA" #N's replaced I's from Jonah

allOrients <- function(primer) {
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna))
  return(sapply(orients, toString))
}

FWD.orients <- allOrients(FWD)
FWD.orients
REV.orients <- allOrients(REV)
REV.orients
```

PrimerHits function is looking for the primers in all orientations in the sequences.

```{r}
primerHits <- function(primer, fn){
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))

```

This checks that we can use cutadapt

```{r}
system2(cutadapt, args = "--version")
```

Running cutadapt. Permission denied on raven but works on laptop

```{r}
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

R1.flags <- paste0("-g", " ^", FWD)
R2.flags <- paste0("-G", " ^", REV)

for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags,
                             "-m 1", #discards reads having length zero bp
                             "--discard-untrimmed",
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], #output files
                             fnFs[i], fnRs[i])) #input files
}
```


# Check Quality after Removing Primers

Path to fastq files after trimming primers 

```{r}
path.cut <- "../Data/cutadapt"
list.files(path.cut)
```

Sorting forward and reverse reads based on file name pattern. save as objects

```{r}

fnFs_cut <- sort(list.files(path.cut, pattern=".R1.fastq", full.names = TRUE))
fnRs_cut <- sort(list.files(path.cut, pattern=".R2.fastq", full.names = TRUE))

sample.names <- str_replace(basename(fnFs), ".1.R1.fastq","")

```

Check for primers in all orientations in cut sequences

```{r}
primerHits <- function(primer, fn){
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_cut[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_cut[[1]]))

```


This shows a plot of the quality of the forward reads

```{r}
plotQualityProfile(fnFs_cut[1:143],aggregate = TRUE)
```

This plot shows quality of the reverse reads

```{r}
plotQualityProfile(fnRs_cut[1:143],aggregate = TRUE)
```

# Filter and trim

```{r}

filtF1s <- file.path(path.cut, "filtered", paste0(sample.names, ".R1.filt.fastq.gz"))
filtR1s <- file.path(path.cut, "filtered", paste0(sample.names, ".R2.filt.fastq.gz"))
names(filtF1s) <- sample.names
names(filtR1s) <- sample.names

out <- filterAndTrim(fnFs_cut, filtF1s, fnRs_cut, filtR1s, truncLen=c(200,200),
                      maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

head(out)

```


```{r Check quality after trimming}
plotQualityProfile(filtF1s[1:5])

plotQualityProfile(filtR1s[1:5])
```
For these plots, the green line is the mean quality score at that position,
the orange lines are the quartiles (solid for median, dashed for 25% and 75%)
and the red line represents the proportion of reads existing at that position.
```{r}
#after removing primers, trimming, filtering

#Forward
qualplotF <- plotQualityProfile(
  filtF1s[1:143],
  aggregate = TRUE
)
qualplotF

#Reverse
qualplotR <- plotQualityProfile(
  filtR1s[1:143],
  aggregate = TRUE
)
qualplotR
```

Here we use a portion of the data to determine error rates. These error rates
will be used in the next (denoising) step to narrow down the sequences to a
reduced and corrected set of unique sequences

```{r learning errors}
errF <- learnErrors(
  filtF1s,
  nbases = 1e+08,
  errorEstimationFunction = loessErrfun,
  multithread = TRUE,
  randomize = FALSE,
  MAX_CONSIST = 10,
  OMEGA_C = 0,
  qualityType = "Auto",
  verbose = FALSE
)

errR <- learnErrors(
  filtR1s, 
  nbases = 1e+08,
  errorEstimationFunction = loessErrfun,
  multithread = TRUE,
  randomize = FALSE,
  MAX_CONSIST = 10,
  OMEGA_C = 0,
  qualityType = "Auto",
  verbose = FALSE
)
```

We can visualize the estimated error rates to make sure they don't look too
crazy. The red lines are error rates expected under the "...nominal defintion
of the Q-score." The black dots are "...observed error rates for each
consensus quality score." The black line shows the "...estimated error rates
after convergence of the machine-learning algorithm." I think the main things
to look at here are to make sure that each black line is a good fit to the
observed error rates, and that estimated error rates decrease with increased
quality.

```{r}
plotErrors(errF, nominalQ=TRUE)
```

I'm not sure if I need this step?

```{r dereplicate}

derepF1s <- derepFastq(filtF1s, verbose = TRUE)
derepR1s <- derepFastq(filtR1s, verbose = TRUE)

```

This applies the "core sample inference algorithm" (i.e. denoising) in dada2
to get corrected unique sequences. The two main inputs are the first, which is
the filtered sequences (filtFs), and "err =" which is the error file from
learnErrors (effF).

```{r dadaing}

dadaF1s <- dada(derepF1s, err = errF, multithread = TRUE)

dadaR1s <- dada(derepR1s, err = errR, multithread = TRUE)

dadaF1s[[1]]
```

# Merge paired reads

```{r}
mergers <- mergePairs(dadaF1s, filtF1s, dadaR1s, filtR1s, 
                      minOverlap = 12, maxMismatch = 3, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

# Construct sequence table

construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}
seqtab <- makeSequenceTable(mergers)

# This describes the dimensions of the table just made
dim(seqtab)

```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants.

```{r}
#Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

```
If you want to remove certain lengths, use this code

```{r}
# In this example, we only keep reads between 298 and 322 bp in length.
seqtab313 <- seqtab[,nchar(colnames(seqtab)) %in% 250:322]
dim(seqtab313)
table(nchar(getSequences(seqtab313)))
```


# Remove Chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r RemovingChimeras, message=F}

seqtab.nochim <- removeBimeraDenovo(seqtab313, method="consensus", multithread=TRUE)

dim(seqtab.nochim)

```

```{r}
# This looks at the % of samples that were not chimeras.
sum(seqtab.nochim)/sum(seqtab313)

```

# Track the reads through the pipeline.

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaF1s, getN), sapply(dadaR1s, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

This exports a sequence-table: columns of ASV's, rows of samples, and values = number of reads.

```{r}
write.table(
  seqtab.nochim,
  file="../Output/Lionfish_sequence-table.tsv",
  quote = FALSE,
  sep="\t",
  row.names = TRUE,
  col.names = NA
)
```

```{r}
seqtab.nochim.tb <- as_tibble(seqtab.nochim, rownames = "sample")
```

```{r}
# The sequence-table has a column with sample names, and N columns of ASV's
# containing count values. We want all the count data to be in a single column,
# so we use a tidyr command called "pivot_longer" to make the table "tall",
# which means the table goes from 17x2811 to 47770x3 for example
# (47770 = 2810 x 17. 2810 instead of 2811 because the first column of the
# original table contains sample names, not counts). This makes the table tidier
# (meaning that each column is now a true variable).

seqtab.nochim.tall <- seqtab.nochim.tb %>%
  pivot_longer (
    !sample,
    names_to = "ASV",
    values_to = "count"
  )

# Look at your new table.
head(seqtab.nochim.tall)
# Look at the dimensions of this table.
dim(seqtab.nochim.tall)

# Remove rows with sequence counts = 0. This removes any samples in which a
# particular ASV was not found.
seqtab.nochim.tall.nozero <- subset(seqtab.nochim.tall, count != 0)
# Look at the dimensions of this table to compare to the previous.
dim(seqtab.nochim.tall.nozero)

# Export Sequence-List Table. This includes 3 columns: sample name, ASV
# sequence, and read count for that sample name ASV combo. This is a tidy table
# (each column a variable), and is a good way to include all the possible data
# involved.
write.table(
  seqtab.nochim.tall.nozero,
  file="../Output/Lionfish_sequence_SeqList_Tall.tsv",
  quote = FALSE,
  sep="\t",
  row.names = FALSE
)
```


